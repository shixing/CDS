#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Handout 1 for Distributed Semantics
\end_layout

\begin_layout Author
Xing Shi
\end_layout

\begin_layout Section
Exploiting Similarities among Languages for Machine Translation
\end_layout

\begin_layout Standard
For each language, learn a word vector model.
 Then learn a translation matrix of these two space.
 
\end_layout

\begin_layout Standard

\series bold
OR, can we learn the two language straightly in a single space ? What will
 happen ?
\end_layout

\begin_layout Section
Random Thoughts
\end_layout

\begin_layout Standard
When we train a word-vector, we just try to predict the surrounding words.
 What if we train the word vectors to predict the surrounding words on the
 dependency tree ? Would that have more regularity on syntactic and on semantic
 ? (The assumption is that, the differences or direction we want the features
 to help, should be conveyed by the training data.
 )
\end_layout

\begin_layout Standard
\noindent
Syntactic Based Machine Translation, they tend to use the syntactic information
 when combine or generating the new sentence to make it grammatically.
 Can we have a better phrase? Yes, if the word embedding's contains the
 syntactic regularity, we can 
\series bold
enrich the phrase list and re-rank it
\series default
.
\end_layout

\begin_layout Standard
--
\end_layout

\begin_layout Standard
\noindent
Traditional Neutral Network's pharse table, just use the count/divide method
 to score the phrase.
 Now, neutral networks just rescore the phrase table, basically, using self-lear
ned features to feed a log-linear classifier.
 Not, could we use a better, human engineered feature set to re-score the
 phrase table ? 
\end_layout

\begin_layout Standard
--
\end_layout

\begin_layout Section
\noindent
Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical
 Machine Translation
\end_layout

\begin_layout Standard
It's really complicated, and only used the rescore methods.
 This is not as good as the ACL 2014's best paper.
 Even though, it learns some word-embeddings.
 How to use the word-embedding suitably, is really the key challenge.
 
\end_layout

\end_body
\end_document
